@article{NamLeLeoYuZhangKewenLiaoShiruiPan2025,
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/abs/2510.14299},
author = {{Nam Le, Leo Yu Zhang, Kewen Liao, Shirui Pan}, Wei Luo},
eprint = {/arxiv.org/abs/2510.14299},
journal = {arXiv},
number = {2510.14299},
primaryClass = {https:},
title = {{TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening}},
url = {https://arxiv.org/abs/2510.14299},
year = {2025}
}
@article{Cheng-YiLeeYu-HsuanChiangZhong-YouWuChia-MuYu2025,
archivePrefix = {arXiv},
arxivId = {arxiv.org/abs/2408.11679},
author = {{Cheng-Yi Lee, Yu-Hsuan Chiang, Zhong-You Wu, Chia-Mu Yu}, Chun-Shien Lu},
eprint = {abs/2408.11679},
journal = {arXiv},
number = {2408.11679},
primaryClass = {arxiv.org},
title = {{BadVim: Unveiling Backdoor Threats in Visual State Space Model}},
url = {https://arxiv.org/abs/2408.11679},
year = {2025}
}
@article{10304184,
author = {Guo, Wei and Tondi, Benedetta and Barni, Mauro},
doi = {10.1109/TIFS.2023.3329426},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {Training;Feature extraction;Clustering algorithms;},
pages = {970--984},
title = {{Universal Detection of Backdoor Attacks via Density-Based Clustering and Centroids Analysis}},
volume = {19},
year = {2024}
}
@inproceedings{10.1007/978-3-031-73033-7_15,
abstract = {Deep neural networks (DNNs) have demonstrated effectiveness in various fields. However, DNNs are vulnerable to backdoor attacks, which inject a unique pattern, called trigger, into the input to cause misclassification to an attack-chosen target label. While existing works have proposed various methods to mitigate backdoor effects in poisoned models, they tend to be less effective against recent advanced attacks. In this paper, we introduce a novel post-training defense technique UNIT that can effectively eliminate backdoor effects for a variety of attacks. In specific, UNIT approximates a unique and tight activation distribution for each neuron in the model. It then proactively dispels substantially large activation values that exceed the approximated boundaries. Our experimental results demonstrate that UNIT outperforms 7 popular defense methods against 14 existing backdoor attacks, including 2 advanced attacks, using only 5{\%} of clean training data. UNIT is also cost efficient. The code is accessible at https://github.com/Megum1/UNIT.},
address = {Cham},
author = {Cheng, Siyuan and Shen, Guangyu and Zhang, Kaiyuan and Tao, Guanhong and An, Shengwei and Guo, Hanxi and Ma, Shiqing and Zhang, Xiangyu},
booktitle = {Computer Vision -- ECCV 2024},
editor = {Leonardis, Ale{\v{s}} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"{u}}l},
isbn = {978-3-031-73033-7},
pages = {262--281},
publisher = {Springer Nature Switzerland},
title = {{UNIT: Backdoor Mitigation via Automated Neural Distribution Tightening}},
year = {2025}
}
@article{LU2022102819,
abstract = {As a new distributed machine learning framework, Federated Learning (FL) effectively solves the problems of data silo and privacy protection in the field of artificial intelligence. However, for its independent devices, heterogeneous data and unbalanced data distribution, it is more vulnerable to adversarial attack, especially backdoor attack. In this paper, we investigate typical backdoor attacks in FL, containing model replacement attack and adaptive backdoor attack. Based on attack initiating round, we divide backdoor attack into convergence-round attack and early-round attack. In addition, we respectively design a defense scheme with model pre-aggregation and similarity measurement to detect and remove backdoor model under convergence-round attack and a defense scheme with backdoor neuron activation to remove backdoor under early-round attack. Experiments and performance analysis show that compared to benchmark schemes, our defense scheme with similarity measurement obtains the highest backdoor detection accuracy under model replacement attack (25% increase) and adaptive backdoor attack (67% increase) at the convergence round. Moreover, detection effect is the most stable. Compared to defense of participant-level differential privacy and adversarial training, our defense scheme with backdoor neuron activation can rapidly remove malicious effects of backdoor without reducing the main task accuracy under early-round attack. Thus, the robustness of FL can be improved greatly with our defense schemes. We make our key codes public at Github https://github.com/lsw3130104597/Backdoor_detection.},
author = {Lu, Shiwei and Li, Ruihu and Liu, Wenbin and Chen, Xuan},
doi = {https://doi.org/10.1016/j.cose.2022.102819},
issn = {0167-4048},
journal = {Computers & Security},
keywords = {Abnormal model detection,Adaptive backdoor attack,Backdoor neuron activation,Federated learning,Model replacement attack,Model similarity measurement},
pages = {102819},
title = {{Defense against backdoor attack in federated learning}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822002139},
volume = {121},
year = {2022}
}
@article{QIAN2025104225,
abstract = {Deep neural networks (DNNs) are vulnerable to backdoor attacks, which can leave traces in the model that are detectable by advanced defense methods. In this paper, we examine the limitations of existing backdoor attacks and defense methods, and propose a scapegoat attack framework designed to divert the attention of defenses and shield genuine backdoor attacks from detection. Our framework leverages channel activation manipulation techniques, comprising three key components: scapegoat, infiltrator, and separation. This allows our genuine backdoor attack to successfully evade defense mechanisms and overcome previously impenetrable defenses while maintaining a high attack success rate. The framework is versatile, enabling the creative configuration of base attack and scapegoat setups. We apply the framework in static, dynamic attack, and clean-label attacks scenarios, demonstrating its efficacy against various advanced defense methods on three different datasets.},
author = {Qian, Yaguan and Lian, Zejie and Li, Yiming and Wang, Wei and Gu, Zhaoquan and Wang, Bin and Zhang, Yanchun},
doi = {https://doi.org/10.1016/j.cose.2024.104225},
issn = {0167-4048},
journal = {Computers & Security},
keywords = {Backdoor attack,Image classification,Network security,Scapegoat,Security models},
pages = {104225},
title = {{Evading backdoor defenses: Concealing genuine backdoors through scapegoat strategy}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005315},
volume = {150},
year = {2025}
}
@article{Prasad2024,
author = {Prasad, Arvind and Chandra, Shalini},
doi = {10.1016/j.cose.2023.103545},
issn = {01674048},
journal = {Computers \& Security},
pages = {103545},
title = {{PhiUSIIL: A diverse security profile empowered phishing URL detection framework based on similarity index and incremental learning}},
volume = {136},
year = {2024}
}
@inproceedings{10.1145/3474376.3487287,
abstract = {Recent automobiles use image sensors to take in the physical world information, and deep neural networks (DNNs) are used to recognize the surroundings to control the vehicles. Adversarial examples and backdoor attacks that induce misclassification by tampering with input images to DNN have been proposed as methods of attacking DNNs. As an example of attacks on DNNs equipped in automobiles, a method has been reported in which an adversarial mark is added to input images by physically putting a sticker on a road sign. However, the method reduces reproducibility due to the influence of the shooting environment. The tampering area needs to be increased to improve reproducibility. However, these large marks are easily seen by people.We propose a method of adding an adversarial mark for triggering backdoor attacks by a fault injection attack on the Mobile Industry Processor Interface (MIPI), which is the popular CMOS image sensor interface. This method can increase the reproducibility of attacks. We attack the MIPI to add adversarial marks to the input image. Two attack drivers are electrically connected to the MIPI data lane in our attack system. Most image signals are transferred from the sensor to the processor by canceling the attack signal. Then, the adversarial mark is added by activating the attack signal. An adversary aiming to carry out backdoor attacks mixes poison data, which consists of images tampered with adversarial marks at specific locations and of adversarial target classes, into a training dataset. The backdoor model classifies images with adversarial marks into an adversarial target class and other images into the correct classes.We conducted backdoor attack experiments with the MNIST dataset for handwritten digit recognition and the German Traffic Sign Recognition Benchmark (GTSRB) dataset for traffic sign recognition. Attacks on MNIST had a 91\% success rate with 1\% poison data, and attacks on GTSRB had a 92\% success rate with 5.1\% poison data.},
address = {New York, NY, USA},
author = {Oyama, Tatsuya and Okura, Shunsuke and Yoshida, Kota and Fujino, Takeshi},
booktitle = {Proceedings of the 5th Workshop on Attacks and Solutions in Hardware Security},
doi = {10.1145/3474376.3487287},
isbn = {9781450386623},
keywords = {backdoor attack,deep neural networks,fault injection attack,image sensor interface,mipi},
pages = {63--72},
publisher = {Association for Computing Machinery},
series = {ASHES '21},
title = {{Backdoor Attack on Deep Neural Networks Triggered by Fault Injection Attack on Image Sensor Interface}},
url = {https://doi.org/10.1145/3474376.3487287},
year = {2021}
}
@article{SHAO2021102433,
abstract = {Deep neural networks (DNNs) have been recently shown to be vulnerable to backdoor attacks. The infected model performs well on benign testing samples, however, the attacker can trigger the infected model to misbehave by the backdoor. In the field of natural language processing (NLP), some backdoor attack methods have been proposed, and achieved high attack success rates on a variety of popular models. However, researches on the defense of textual backdoor attacks are lacking and the defense effects are bad at present. In this paper, we propose an effective textual backdoor defense model, namely BDDR, which contains two steps: (1) detecting suspicious words in the sample and (2) reconstructing the original text by deletion or replacement. In the replacement part, we use the pre-trained masking language model taking BERT as an example to generate replacement words. We conduct exhaustive experiments to evaluate our proposed defense model by defending against various backdoor attacks on two infected models trained using two benchmark datasets. Overall, BDDR reduces the attack success rate of word-level backdoor attacks by more than 90%, and reduces the attack success rate of sentence-level backdoor attacks by more than 60%. The experimental results show that our proposed method can always significantly reduce the attack success rate compared with the baseline method.},
author = {Shao, Kun and Yang, Junan and Ai, Yang and Liu, Hui and Zhang, Yu},
doi = {https://doi.org/10.1016/j.cose.2021.102433},
issn = {0167-4048},
journal = {Computers & Security},
keywords = {Adversarial Machine Learning,Backdoor Attacks,Backdoor Defenses,Deep Neural Networks,Natural Language Processing},
pages = {102433},
title = {{BDDR: An Effective Defense Against Textual Backdoor Attacks}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002571},
volume = {110},
year = {2021}
}
@article{KAVIANI2021651,
abstract = {Deep learning techniques have become significantly prevalent in many real-world problems including a variety of detection, recognition, and classification tasks. To obtain high-performance neural networks, an enormous amount of training datasets, memory, and time-consuming computations are required which has increased the demands for outsource training among users. As a result, the machine-learning-as-a-service(MLaaS) providers or a third party can gain an opportunity to put the model's security at risk by training the model with malicious inputs. The malicious functionality inserted into the neural network by the adversary will be activated in the presence of specific inputs. These kinds of attacks to neural networks, called trojan or backdoor attacks, are very stealthy and hard to detect because they do not affect the network performance on clean datasets. In this paper, we refer to two important threat models and we focus on the detection and mitigation techniques against these types of attacks on neural networks which has been proposed recently. We summarize, discuss, and compare the defense methods and their corresponding results.},
author = {Kaviani, Sara and Sohn, Insoo},
doi = {https://doi.org/10.1016/j.neucom.2020.07.133},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Backdoor attacks,Deep learning,Defense,Trojan attacks},
pages = {651--667},
title = {{Defense against neural trojan attacks: A survey}},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220316350},
volume = {423},
year = {2021}
}
@article{XUE2022102726,
abstract = {Deep neural networks (DNN) models have been widely applied in many tasks. However, recent researches have shown that DNN models are vulnerable to backdoor attacks. A number of backdoor attacks on DNN models have been proposed, but almost all the existing backdoor attacks are digital backdoor attacks. However, when launching backdoor attacks in the real physical world, the attack performance will be severely degraded due to a variety of physical constraints. In this paper, we propose a robust physical backdoor attack method, named physical transformations for backdoors (PTB), to implement the backdoor attacks against DNN models in real physical world. To the best of our knowledge, we are the first to propose a robust physical backdoor attack with real physical triggers working under complex physical conditions. We use real physical objects as the triggers, and perform a series of physical transformations on the injected backdoor instances during model training, so as to simulate various transformations that a backdoor instance may experience in real physical world, thus ensures its physical robustness. Experimental results on face recognition model demonstrate that, compared with normal backdoor attacks without PTB, the proposed attack method can significantly improve the attack performance in real physical world. Under various complex physical conditions, by injecting only a very small ratio (0.5%) of backdoor instances, the attack success rate of physical backdoor attack with the PTB method is 78% (Square), 82% (Triangle), 79% (Glasses) on YouTube Aligned Face dataset, and 78% (Square), 86% (Triangle), 85% (Glasses) on VGG Face dataset, respectively, while the attack success rate of backdoor attacks without PTB is only 5% (Square), 11% (Triangle), 9% (Glasses) on YouTube Aligned Face dataset and 21% (Square), 20% (Triangle), 13% (Glasses) on VGG Face dataset, respectively. Meanwhile, the proposed method will not affect the normal performance of the DNN model. In addition, experimental results also demonstrate that the proposed robust physical backdoor attack can evade the detection of three backdoor defense methods.},
author = {Xue, Mingfu and He, Can and Wu, Yinghao and Sun, Shichang and Zhang, Yushu and Wang, Jian and Liu, Weiqiang},
doi = {https://doi.org/10.1016/j.cose.2022.102726},
issn = {0167-4048},
journal = {Computers & Security},
keywords = {Artificial intelligence securitys,Deep neural networks,Face recognition,Physical backdoor attack,Physical transformations},
pages = {102726},
title = {{PTB: Robust physical backdoor attacks against deep neural networks in real world}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404822001213},
volume = {118},
year = {2022}
}
@article{ZHANG2025121562,
abstract = {Deep neural networks (DNNs) have excellent performance in various applications, especially for image classification tasks. However, DNNs also face the threat of backdoor attacks. Backdoor attacks embed a hidden backdoor into a model, after which the infected model can achieve correct classification on benign images, while incorrectly classify the images with the backdoor triggers as the target label. To obtain a clean model from a backdoor dataset, we propose a Kalman filtering based multi-scale inactivation scheme, which can effectively remove poison data in a poison dataset and obtain a clean model. Every sample in the suspicious training dataset will be judged by multi-scale inactivation and obtain a series of judging results, then data fusion is conducted using kalman filtering to determine whether it is a poison sample. To further improve the performance, a trigger localization and target determination based scheme is proposed. Extensive experiments are conducted to demonstrate the superior effectiveness of the proposed method. The results show that the proposed methods can remove poison samples effectively, and achieve greater than 99% recall rate, and the attack success rate of the retrained clean model is smaller than 1%.},
author = {Zhang, Anqing and Chen, Honglong and Wang, Xiaomeng and Li, Junjian and Gao, Yudong and Wang, Xingang},
doi = {https://doi.org/10.1016/j.ins.2024.121562},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {Backdoor attack,DNNs,Dataset preparation,Kalman filtering},
pages = {121562},
title = {{Defending against backdoor attack on deep neural networks based on multi-scale inactivation}},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524014762},
volume = {690},
year = {2025}
}
@inproceedings{10646715,
author = {Liu, Junrui and Kretz, Ian and Liu, Hanzhi and Tan, Bryan and Wang, Jonathan and Sun, Yi and Pearson, Luke and Miltner, Anders and Dillig, Işıl and Feng, Yu},
booktitle = {2024 IEEE Symposium on Security and Privacy (SP)},
doi = {10.1109/SP54263.2024.00078},
keywords = {Privacy;Circuits;Buildings;Computer bugs;Manuals;L},
pages = {1741--1759},
title = {{Certifying Zero-Knowledge Circuits with Refinement Types}},
year = {2024}
}
@article{JIN2025100326,
abstract = {Deep neural networks (DNNs) have found extensive applications in safety-critical artificial intelligence systems, such as autonomous driving and facial recognition systems. However, recent research has revealed their susceptibility to backdoors maliciously injected by adversaries. This vulnerability arises due to the intricate architecture and opacity of DNNs, resulting in numerous redundant neurons embedded within the models. Adversaries exploit these vulnerabilities to conceal malicious backdoor information within DNNs, thereby causing erroneous outputs and posing substantial threats to the efficacy of DNN-based applications. This article presents a comprehensive survey of backdoor attacks against DNNs and the countermeasure methods employed to mitigate them. Initially, we trace the evolution of the concept from traditional backdoor attacks to backdoor attacks against DNNs, highlighting the feasibility and practicality of generating backdoor attacks against DNNs. Subsequently, we provide an overview of notable works encompassing various attack and defense strategies, facilitating a comparative analysis of their approaches. Through these discussions, we offer constructive insights aimed at refining these techniques. Finally, we extend our research perspective to the domain of large language models (LLMs) and synthesize the characteristics and developmental trends of backdoor attacks and defense methods targeting LLMs. Through a systematic review of existing studies on backdoor vulnerabilities in LLMs, we identify critical open challenges in this field and propose actionable directions for future research.},
author = {Jin, Ling-Xin and Jiang, Wei and Wen, Xiang-Yu and Lin, Mei-Yu and Zhan, Jin-Yu and Zhou, Xing-Zhi and Habtie, Maregu Assefa and Werghi, Naoufel},
doi = {https://doi.org/10.1016/j.jnlest.2025.100326},
issn = {1674-862X},
journal = {Journal of Electronic Science and Technology},
keywords = {Backdoor attacks,Backdoor defenses,Deep neural networks,Large language model},
number = {3},
pages = {100326},
title = {{A survey of backdoor attacks and defences: From deep neural networks to large language models}},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X25000278},
volume = {23},
year = {2025}
}
@article{XIANG2021102280,
abstract = {Backdoor data poisoning (a.k.a. Trojan attack) is an emerging form of adversarial attack usually against deep neural network image classifiers. The attacker poisons the training set with a relatively small set of images from one (or several) source class(es), embedded with a backdoor pattern and labeled to a target class. For a successful attack, during operation, the trained classifier will: 1) misclassify a test image from the source class(es) to the target class whenever the backdoor pattern is present; 2) maintain high classification accuracy for backdoor-free test images. In this paper, we make a breakthrough in defending backdoor attacks with imperceptible backdoor patterns (e.g. watermarks) before/during the classifier training phase. This is a challenging problem because it is a priori unknown which subset (if any) of the training set has been poisoned. We propose an optimization-based reverse engineering defense that jointly: 1) detects whether the training set is poisoned; 2) if so, accurately identifies the target class and the training images with the backdoor pattern embedded; and 3) additionally, reverse engineers an estimate of the backdoor pattern used by the attacker. In benchmark experiments on CIFAR-10 (as well as four other data sets), considering a variety of attacks, our defense achieves a new state-of-the-art by reducing the attack success rate to no more than 4.9% after removing detected suspicious training images.},
author = {Xiang, Zhen and Miller, David J and Kesidis, George},
doi = {https://doi.org/10.1016/j.cose.2021.102280},
issn = {0167-4048},
journal = {Computers & Security},
keywords = {Adversarial learning,Backdoor,Deep neural network,Image classification,Reverse engineering,Trojan},
pages = {102280},
title = {{Reverse engineering imperceptible backdoor attacks on deep neural networks for detection and training set cleansing}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821001048},
volume = {106},
year = {2021}
}
@inproceedings{Wang2019,
author = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
doi = {10.1109/SP.2019.00031},
pages = {707--723},
title = {{Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks}},
year = {2019}
}
@inproceedings{Zhu2023,
author = {Zhu, Ming and Wei, Wei and Chen, Tianlong},
booktitle = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV51070.2023.00414},
pages = {4466--4476},
title = {{Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization}},
year = {2023}
}
@inproceedings{Huang2023,
author = {Huang, Hsin-Pai and Ma, Xingjun and Erfani, Sarah Monazam and Bailey, James},
booktitle = {International Conference on Learning Representations (ICLR)},
doi = {@inproceedings{ huang2023distilling, title={Distilling Cognitive Backdoor Patterns within an Image}, author={Hanxun Huang and Xingjun Ma and Sarah Monazam Erfani and James Bailey}, booktitle={The Eleventh International Conference on Learning Representations }, year={2023}, url={https://openreview.net/forum?id=S3D9NLzjnQ5} }},
title = {{Distilling Cognitive Backdoor Patterns Within an Image}},
url = {https://openreview.net/forum?id=S3D9NLzjnQ5},
year = {2023}
}
@inproceedings{Li2021,
author = {Li, Yiming and Li, Yong and Wu, Baoyuan and Li, Li and He, Ran and Lyu, Siwei},
booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV48922.2021.01615},
pages = {16463--16472},
title = {{Invisible Backdoor Attack with Sample-Specific Triggers}},
year = {2021}
}
@inproceedings{Yun2019,
abstract = {We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with “slightest” nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general “no spurious local minima” is a property limited to deep linear networks, and insights obtained from linear networks may not be robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on spurious local optima in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.},
archivePrefix = {arXiv},
arxivId = {1802.03487},
author = {Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1802.03487},
title = {{Small nonlinearities in activation functions create bad local minima in neural networks}},
year = {2019}
}
@inproceedings{Hayase2021,
author = {Hayase, Jonathan and Kong, Weihao and Somani, Rohan and Oh, Sewoong},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {4129--4139},
publisher = {PMLR},
title = {{SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics}},
url = {https://proceedings.mlr.press/v139/hayase21a.html},
year = {2021}
}
@inproceedings{Liu2018,
author = {Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
booktitle = {Research in Attacks, Intrusions, and Defenses (RAID)},
doi = {10.1007/978-3-030-00470-5_13},
pages = {273--294},
title = {{Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks}},
year = {2018}
}
@inproceedings{Nguyen2021,
author = {Nguyen, Anh and Tran, Anh},
booktitle = {International Conference on Learning Representations (ICLR)},
title = {{WaNet -- Imperceptible Warping-based Backdoor Attack}},
url = {https://openreview.net/pdf?id=eEn8KTtJOx},
year = {2021}
}
@inproceedings{Andriushchenko2022,
author = {Andriushchenko, Maksym and Flammarion, Nicolas},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {639--668},
title = {{Towards Understanding Sharpness-Aware Minimization}},
url = {https://proceedings.mlr.press/v162/andriushchenko22a/andriushchenko22a.pdf},
year = {2022}
}
@misc{Chen2017,
author = {Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
doi = {10.48550/arXiv.1712.05526},
title = {{Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning}},
year = {2017}
}
@inproceedings{Kwon2021,
author = {Kwon, Jeongseob and Kim, Jaehyung and Park, Hyeongseok and Choi, In-kyu},
booktitle = {International Conference on Machine Learning (ICML)},
pages = {5905--5914},
title = {{ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks}},
url = {https://proceedings.mlr.press/v139/kwon21b.html},
year = {2021}
}
@inproceedings{Tramer2020,
author = {Tram{\`{e}}r, Florian and Carlini, Nicholas and Brendel, Wieland and Madry, Aleksander},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
pages = {1633--1645},
title = {{On Adaptive Attacks to Adversarial Example Defenses}},
url = {https://proceedings.neurips.cc/paper/2020/hash/11f38f8ecd71867b42433548d1078e38-Abstract.html},
year = {2020}
}
@inproceedings{Du2022,
author = {Du, Jiachen and Jiang, Hui and Veit, Andreas and Beirami, Ahmad},
booktitle = {International Conference on Learning Representations (ICLR)},
title = {{Efficient Sharpness-Aware Minimization for Improved Training of Neural Networks}},
url = {https://liangli-zhen.github.io/assets/pdf/ICLR2022_efficient_sharpness_aware_mini.pdf},
year = {2022}
}
@inproceedings{Li2021a,
author = {Li, Yiming and Lyu, Xin and Koren, Noga and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
title = {{Anti-Backdoor Learning: Training Clean Models on Poisoned Data}},
url = {https://papers.neurips.cc/paper_files/paper/2021/file/7d38b1e9bd793d3f45e0e212a729a93c-Paper.pdf},
year = {2021}
}
@article{Li2022,
author = {Li, Shuo and Xue, Minghao and Zhao, Benjamin Zi Hao and Zhu, Haining and Zhang, Xiaofeng},
doi = {10.1109/TDSC.2021.3108802},
journal = {IEEE Transactions on Dependable and Secure Computing},
number = {5},
pages = {2959--2972},
title = {{Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization}},
volume = {19},
year = {2022}
}
@article{Gu2019,
author = {Gu, Tianyu and Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
doi = {10.1109/ACCESS.2019.2909068},
journal = {IEEE Access},
pages = {47230--47244},
title = {{BadNets: Evaluating Backdooring Attacks on Deep Neural Networks}},
volume = {7},
year = {2019}
}
@article{Wu2023,
author = {Wu, Dong and Wang, Yuhang},
doi = {10.1109/TPAMI.2023.3296395},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {13838--13849},
title = {{Adversarial Neuron Pruning Purifies Backdoored Deep Models}},
volume = {45},
year = {2023}
}
@inproceedings{Li2021b,
author = {Li, Yiming and Lyu, Xin and Koren, Noga and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},
booktitle = {International Conference on Learning Representations (ICLR)},
title = {{Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks}},
year = {2021}
}
@inproceedings{Saha2020,
author = {Saha, Aniruddha and Subramanya, Amrita and Pirsiavash, Hamed},
booktitle = {AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v34i07.6871},
number = {07},
pages = {11957--11965},
title = {{Hidden Trigger Backdoor Attacks}},
volume = {34},
year = {2020}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.1.1},
journal = {Neural Computation},
number = {1},
pages = {1--42},
title = {{Flat Minima}},
volume = {9},
year = {1997}
}
@article{Goldblum2023,
author = {Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Goldstein, Tom},
doi = {10.1109/TPAMI.2022.3162397},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {2},
pages = {1563--1580},
title = {{Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses}},
volume = {45},
year = {2023}
}
@article{Liu2020,
author = {Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Wei and Zhang, Xiangyu},
doi = {10.1109/TDSC.2020.3032076},
journal = {IEEE Transactions on Dependable and Secure Computing},
number = {5},
pages = {2436--2449},
title = {{Trojaning Attack on Neural Networks}},
volume = {18},
year = {2020}
}
@inproceedings{Salem2022,
author = {Salem, Ahmed and Wen, Rong and Backes, Michael and Ma, Shiqing and Zhang, Yang},
booktitle = {2022 IEEE Symposium on Security and Privacy (SP)},
doi = {10.1109/SP46214.2022.00049},
pages = {1952--1969},
title = {{Dynamic Backdoor Attacks Against Machine Learning Models}},
year = {2022}
}
